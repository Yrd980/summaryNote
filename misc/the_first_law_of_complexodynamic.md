## introduce [link](https://scottaaronson.blog/?p=762)

That link is to **Scott Aaronson’s blog “Shtetl-Optimized”**, specifically his famous 2011 post titled **“Why Philosophers Should Care About Computational Complexity.”**

It’s essentially the online preprint of his lecture notes and later book chapter, where he argues that computational complexity theory has deep implications for classic questions in philosophy, like:

* **What can be known or proven?** (Epistemology, limits of knowledge)
* **What counts as an explanation?**
* **Free will vs determinism** (Can unpredictability from computational hardness matter?)
* **Nature of mathematical truth** (complexity shapes what kinds of proofs are feasible)
* **The role of randomness and pseudorandomness** in philosophy of science

He tries to convince philosophers that computational complexity—especially results like **P vs NP**, **interactive proofs**, and **quantum computing limits**—isn’t just math trivia, but changes how we should think about reasoning, knowledge, and possibility.

---

In that blog post, he introduces the **First Law of Complexodynamics** as an analogy to the **Second Law of Thermodynamics**:

> **First Law of Complexodynamics**:
> *“Isolated systems tend to evolve toward states of higher Kolmogorov complexity.”*

---

### What this means

* **Kolmogorov complexity** = the length of the shortest computer program that can describe something (a string, a state, etc.).
* The law says that, just as entropy tends to increase in physics, so too does “algorithmic randomness” or descriptive complexity tend to increase.
* In other words: systems “naturally” become harder to compress, less predictable, and more complex in their patterns.
